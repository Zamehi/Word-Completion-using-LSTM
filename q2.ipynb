{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Remove all stop words to just train on the words\n",
    "- Tokenize the text into words.\n",
    "- Create a vocabulary of unique words and map each word to an integer.\n",
    "- Convert the text into sequences of integers representing words.\n",
    "- Split the text into overlapping sequences of fixed length (e.g., 5 words per sequence).\n",
    "- For each sequence, the target will be the next word in the sequence.\t-> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing():\n",
    "    with open('Shakespeare_plays_dataset/t.txt', 'r') as file:\n",
    "        text = file.read().lower()\n",
    "    new_text = remove_stopwords(text)\n",
    "    return new_text\n",
    "cleaned_text = data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([cleaned_text])       \n",
    "total_words = len(tokenizer.word_index) + 1  # Total no,of unique words\n",
    "word_index = tokenizer.word_index            # Results a dictionary where each word is mapped to a unique index (starting from 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences of words (n-grams)\n",
    "input_sequences = []\n",
    "for line in cleaned_text.split('.'):  # Splitting by sentences\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    \n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 5\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of Suitable Model Architecture and Training the Model with suitable metrics\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X (Features/Words)`: This contains all but the last word in each input sequence. In a sequence of words, X serves as the input for the model, which the model will learn from.\n",
    "- `y (Targets)`: This contains the last word in each input sequence, which serves as the target or the expected output for the model to predict.\n",
    "\n",
    "By setting up X and y this way, you enable the model to learn the relationship between the context (the preceding words) and the prediction (the next word). <br>\n",
    "\n",
    "**One-Hot Encoding**: This converts the target labels (which are integers representing words) into a binary matrix. Each row corresponds to a word, where a 1 indicates the presence of the word at that index, and 0s elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# For each sequence, the target will be the next word in the sequence\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]  # Features and targets\n",
    "y = pd.get_dummies(y).values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(\"Training data shape:\", X_train.shape)\n",
    "# print(\"Training labels shape:\", y_train.shape)\n",
    "# print(\"Testing data shape:\", X_test.shape)\n",
    "# print(\"Testing labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111726, 13886)\n",
      "(27932, 13886)\n",
      "13993\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 72ms/step - accuracy: 0.0166 - loss: 8.0998 - val_accuracy: 0.0215 - val_loss: 7.8308\n",
      "Epoch 2/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 69ms/step - accuracy: 0.0267 - loss: 7.5603 - val_accuracy: 0.0308 - val_loss: 7.8269\n",
      "Epoch 3/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 69ms/step - accuracy: 0.0407 - loss: 7.2058 - val_accuracy: 0.0373 - val_loss: 7.8764\n",
      "Epoch 4/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 72ms/step - accuracy: 0.0573 - loss: 6.7147 - val_accuracy: 0.0407 - val_loss: 8.0222\n",
      "Epoch 5/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 76ms/step - accuracy: 0.0915 - loss: 6.1347 - val_accuracy: 0.0405 - val_loss: 8.2651\n",
      "Epoch 6/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 69ms/step - accuracy: 0.1369 - loss: 5.5389 - val_accuracy: 0.0392 - val_loss: 8.5216\n",
      "Epoch 7/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 56ms/step - accuracy: 0.1924 - loss: 4.9819 - val_accuracy: 0.0381 - val_loss: 8.7914\n",
      "Epoch 8/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 58ms/step - accuracy: 0.2468 - loss: 4.5118 - val_accuracy: 0.0366 - val_loss: 9.0348\n",
      "Epoch 9/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 64ms/step - accuracy: 0.2974 - loss: 4.1197 - val_accuracy: 0.0368 - val_loss: 9.3157\n",
      "Epoch 10/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 62ms/step - accuracy: 0.3437 - loss: 3.7546 - val_accuracy: 0.0363 - val_loss: 9.4635\n",
      "Epoch 11/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 58ms/step - accuracy: 0.3860 - loss: 3.4719 - val_accuracy: 0.0348 - val_loss: 9.6538\n",
      "Epoch 12/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 68ms/step - accuracy: 0.4256 - loss: 3.2109 - val_accuracy: 0.0348 - val_loss: 9.7870\n",
      "Epoch 13/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 58ms/step - accuracy: 0.4597 - loss: 2.9800 - val_accuracy: 0.0338 - val_loss: 9.9253\n",
      "Epoch 14/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 61ms/step - accuracy: 0.4879 - loss: 2.8005 - val_accuracy: 0.0323 - val_loss: 10.0292\n",
      "Epoch 15/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 61ms/step - accuracy: 0.5100 - loss: 2.6626 - val_accuracy: 0.0337 - val_loss: 10.1196\n",
      "Epoch 16/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 59ms/step - accuracy: 0.5340 - loss: 2.5044 - val_accuracy: 0.0339 - val_loss: 10.1932\n",
      "Epoch 17/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 57ms/step - accuracy: 0.5552 - loss: 2.3703 - val_accuracy: 0.0333 - val_loss: 10.2689\n",
      "Epoch 18/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 60ms/step - accuracy: 0.5717 - loss: 2.2760 - val_accuracy: 0.0325 - val_loss: 10.3152\n",
      "Epoch 19/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 63ms/step - accuracy: 0.5908 - loss: 2.1726 - val_accuracy: 0.0330 - val_loss: 10.3613\n",
      "Epoch 20/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 59ms/step - accuracy: 0.6081 - loss: 2.0711 - val_accuracy: 0.0327 - val_loss: 10.3780\n",
      "Epoch 21/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 54ms/step - accuracy: 0.6203 - loss: 1.9869 - val_accuracy: 0.0321 - val_loss: 10.3878\n",
      "Epoch 22/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 53ms/step - accuracy: 0.6260 - loss: 1.9457 - val_accuracy: 0.0322 - val_loss: 10.4018\n",
      "Epoch 23/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 55ms/step - accuracy: 0.6408 - loss: 1.8627 - val_accuracy: 0.0323 - val_loss: 10.4154\n",
      "Epoch 24/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 55ms/step - accuracy: 0.6514 - loss: 1.8071 - val_accuracy: 0.0324 - val_loss: 10.4497\n",
      "Epoch 25/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 58ms/step - accuracy: 0.6606 - loss: 1.7347 - val_accuracy: 0.0313 - val_loss: 10.4495\n",
      "Epoch 26/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 62ms/step - accuracy: 0.6720 - loss: 1.6823 - val_accuracy: 0.0311 - val_loss: 10.4253\n",
      "Epoch 27/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 61ms/step - accuracy: 0.6780 - loss: 1.6504 - val_accuracy: 0.0326 - val_loss: 10.4314\n",
      "Epoch 28/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 61ms/step - accuracy: 0.6863 - loss: 1.5955 - val_accuracy: 0.0323 - val_loss: 10.4214\n",
      "Epoch 29/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 56ms/step - accuracy: 0.6961 - loss: 1.5452 - val_accuracy: 0.0316 - val_loss: 10.4245\n",
      "Epoch 30/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 57ms/step - accuracy: 0.7024 - loss: 1.5113 - val_accuracy: 0.0314 - val_loss: 10.4084\n",
      "Epoch 31/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 57ms/step - accuracy: 0.7086 - loss: 1.4745 - val_accuracy: 0.0324 - val_loss: 10.3929\n",
      "Epoch 32/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 61ms/step - accuracy: 0.7115 - loss: 1.4498 - val_accuracy: 0.0314 - val_loss: 10.3805\n",
      "Epoch 33/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 61ms/step - accuracy: 0.7196 - loss: 1.4102 - val_accuracy: 0.0329 - val_loss: 10.3687\n",
      "Epoch 34/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 62ms/step - accuracy: 0.7253 - loss: 1.3763 - val_accuracy: 0.0315 - val_loss: 10.3689\n",
      "Epoch 35/35\n",
      "\u001b[1m4470/4470\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 61ms/step - accuracy: 0.7317 - loss: 1.3598 - val_accuracy: 0.0317 - val_loss: 10.3646\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "# A Sequential model is initialized, which allows us to build the neural network layer by layer.\n",
    "model = Sequential()\n",
    "\n",
    "# Add embedding layer (converts word indices to dense vectors)\n",
    "model.add(Embedding(input_dim = total_words, output_dim = 300, input_length=max_sequence_len - 1))\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(150, return_sequences=False))\n",
    "model.add(Dropout(0.3))  # Prevent overfitting by randomly droping 30% of the neurons \n",
    "\n",
    "# Add Dense layer with softmax for next word prediction\n",
    "# Dense layer - number of neurons equal to the total number of unique words\n",
    "model.add(Dense(y_test.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping to stop training when validation accuracy stops improving\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=35, batch_size=25, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving the model for later use and creating pickle files to link the model to the User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word: yet\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(model, tokenizer, seed_text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted_probs = model.predict(token_list, verbose=0)\n",
    "    predicted = np.argmax(predicted_probs, axis=-1)\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            return word\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "seed_text = \"Hello, my name could revenge you\"\n",
    "\n",
    "next_word = predict_next_word(model, tokenizer, seed_text, max_sequence_len=6)\n",
    "print(f\"Next word: {next_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Implement the LSTM model in such a way that it predict next few words (3) instead of just one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words: ['could', 'revenge', 'you']\n"
     ]
    }
   ],
   "source": [
    "def predict_next_words(model, tokenizer, seed_text, max_sequence_len, n_words=3):\n",
    "    result = []\n",
    "    for _ in range(n_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=-1)\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                next_word = word\n",
    "                break\n",
    "        result.append(next_word)\n",
    "        seed_text += \" \" + next_word\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "next_words = predict_next_words(model, tokenizer, \"Hello, my name\", max_sequence_len=6, n_words=3)\n",
    "print(\"Next words:\", next_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc5_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
